# -*- coding: utf-8 -*-
"""CAR SALES ANALYSIS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ME643isCcXI2hloDrxaJmhNbhuZqHU7m
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import io

df = pd.read_csv('/content/Car_sales.csv')

print("--- Initial Data Inspection ---")

print("Shape:", df.shape)
print("\nFirst 5 rows:")

print(df.head())
print("\nData Info:")
df.info()

print("\nSummary Statistics (Numerical - initial):")

print(df.describe(include=np.number))
# Only includes columns currently recognized as numeric
print("\nSummary Statistics (Categorical - initial):")
print(df.describe(include='object'))

"""# --- Data Cleaning ---"""

# Rename column
df.rename(columns={'__year_resale_value': 'Year_resale_value'}, inplace=True)
print("Renamed '__year_resale_value' to 'Year_resale_value'")

# Identify columns that should be numeric
numeric_cols_to_convert = [
    'Sales_in_thousands', 'Year_resale_value', 'Price_in_thousands',
    'Engine_size', 'Horsepower', 'Wheelbase', 'Width', 'Length',
    'Curb_weight', 'Fuel_capacity', 'Fuel_efficiency', 'Power_perf_factor'
]

print("\nConverting columns to numeric...")
for col in numeric_cols_to_convert:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Identify all numeric columns after conversion for further operations
numeric_cols = df.select_dtypes(include=np.number).columns.tolist()

# Handle rows with critical missing data after numeric conversion
# Example: Chrysler, Town & Country has many NaNs after conversion for essential features.
# Dropping rows where 'Price_in_thousands' AND 'Horsepower' are both NaN, as these are key.
# You might choose a different strategy based on your specific needs.
print(f"Shape before dropping rows with NaNs in Price & Horsepower: {df.shape}")
df.dropna(subset=['Price_in_thousands', 'Horsepower'], how='all', inplace=True)
print(f"Shape after dropping rows with NaNs in Price or Horsepower: {df.shape}")

# Missing value counts before imputation
print("\nMissing values BEFORE imputation (after initial drop):")
missing_values = df.isnull().sum()
print(missing_values[missing_values > 0])

# Imputation strategy
# For numerical columns, use median (robust to outliers).
print("\nImputing missing numerical values with median...")
for col in numeric_cols:
    if df[col].isnull().any():
        median_val = df[col].median()
        df[col].fillna(median_val, inplace=True)
        print(f"Imputed missing values in '{col}' with median: {median_val:.2f}")

# For categorical columns (e.g., 'Vehicle_type'), use mode.
# Check 'Vehicle_type' (it should be clean based on data, but good practice)
if 'Vehicle_type' in df.columns and df['Vehicle_type'].isnull().any():
    mode_val = df['Vehicle_type'].mode()[0]
    df['Vehicle_type'].fillna(mode_val, inplace=True)
    print(f"Imputed missing values in 'Vehicle_type' with mode: {mode_val}")
else:
    print("'Vehicle_type' has no missing values or does not exist after prior steps.")

# Convert 'Latest_Launch' to datetime
df['Latest_Launch'] = pd.to_datetime(df['Latest_Launch'], errors='coerce')
if df['Latest_Launch'].isnull().any():
    print(f"Warning: {df['Latest_Launch'].isnull().sum()} 'Latest_Launch' dates were NaT after parsing.")
    # Option: fill with a known valid date (e.g., median or mode of valid dates)
    if df['Latest_Launch'].count() > 0: # Check if there are any valid dates to impute from
        valid_dates = df['Latest_Launch'].dropna()
        if not valid_dates.empty:
            impute_date = valid_dates.mode()[0] # Or .median()
            df['Latest_Launch'].fillna(impute_date, inplace=True)
            print(f"Filled NaT in 'Latest_Launch' with: {impute_date.strftime('%Y-%m-%d')}")
        else:
            print("No valid dates in 'Latest_Launch' to use for imputation.")
    else:
        print("All 'Latest_Launch' dates are NaT or column is empty, cannot impute.")

# Check for duplicates
print(f"\nNumber of duplicate rows: {df.duplicated().sum()}")
df.drop_duplicates(inplace=True)
print(f"Shape after dropping duplicates: {df.shape}")
print("\nMissing values AFTER imputation and cleaning:")
final_missing = df.isnull().sum()
print(final_missing[final_missing > 0]) # Show only columns that still have missing values

print("\nData Info after cleaning:")
df.info()

# --- Exploratory Data Analysis (EDA) - Numerical ---
print("\n--- EDA: Numerical Features ---")
print("\nSummary Statistics (Numerical - cleaned):")
print(df[numeric_cols].describe())

# Histograms for numerical features
print("\nPlotting histograms for numerical features...")
df[numeric_cols].hist(bins=15, figsize=(18, 12), layout=(-1, 4)) # Auto layout rows
plt.suptitle("Histograms of Numerical Features", y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

# Box plots for numerical features to check for outliers
print("\nPlotting box plots for numerical features...")
plt.figure(figsize=(20, 15))
for i, col in enumerate(numeric_cols):
    plt.subplot(4, 4, i + 1) # Adjust grid size if more/less numeric_cols
    sns.boxplot(y=df[col])
    plt.title(col, fontsize=10)
    plt.ylabel("") # Remove y-axis label for cleaner look
    plt.xticks(fontsize=8)
    plt.yticks(fontsize=8)
plt.suptitle("Box Plots of Numerical Features", y=1.00, fontsize=16) # Adjusted y for suptitle
plt.tight_layout(rect=[0, 0, 1, 0.97]) # Adjusted rect for suptitle
plt.show()

# --- Exploratory Data Analysis (EDA) - Categorical ---
print("\n--- EDA: Categorical Features ---")
categorical_cols = df.select_dtypes(include='object').columns.tolist()
if 'Latest_Launch' in df.columns and pd.api.types.is_datetime64_any_dtype(df['Latest_Launch']):
    # For EDA purposes, we might not treat datetime as categorical for countplots
    # but you could extract year/month if needed:
    # df['Launch_Year'] = df['Latest_Launch'].dt.year
    # categorical_cols.append('Launch_Year')
    pass # Keep it as datetime for now

for col in categorical_cols:
    if col == 'Model': # Model names are too unique for a meaningful count plot
        print(f"\nValue counts for {col} (Top 10):")
        print(df[col].value_counts().head(10))
        print(f"... and {df[col].nunique() - 10} more unique models.")
        continue

    print(f"\nValue counts for {col}:")
    print(df[col].value_counts())

    # Plot if number of unique values is manageable
    if df[col].nunique() < 30 and df[col].nunique() > 1: # Ensure there's something to plot
        plt.figure(figsize=(12, max(6, df[col].nunique() * 0.3))) # Adjust height based on num categories
        sns.countplot(y=df[col], order=df[col].value_counts().index, palette="viridis")
        plt.title(f"Frequency of {col}", fontsize=14)
        plt.xlabel("Count", fontsize=12)
        plt.ylabel(col, fontsize=12)
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=10)
        plt.tight_layout()
        plt.show()
    elif df[col].nunique() == 1:
        print(f"Skipping plot for {col} as it has only 1 unique value.")
    else:
        print(f"Skipping plot for {col} due to too many unique values ({df[col].nunique()}). Consider top N if needed.")
plt.suptitle("Histograms of Numerical Features", y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

# Box plots for numerical features to check for outliers
print("\nPlotting box plots for numerical features...")
plt.figure(figsize=(20, 15))
for i, col in enumerate(numeric_cols):
    plt.subplot(4, 4, i + 1) # Adjust grid size if more/less numeric_cols
    sns.boxplot(y=df[col])
    plt.title(col, fontsize=10)
    plt.ylabel("") # Remove y-axis label for cleaner look
    plt.xticks(fontsize=8)
    plt.yticks(fontsize=8)
plt.suptitle("Box Plots of Numerical Features", y=1.00, fontsize=16) # Adjusted y for suptitle
plt.tight_layout(rect=[0, 0, 1, 0.97]) # Adjusted rect for suptitle
plt.show()

# --- Exploratory Data Analysis (EDA) - Categorical ---
print("\n--- EDA: Categorical Features ---")
categorical_cols = df.select_dtypes(include='object').columns.tolist()
if 'Latest_Launch' in df.columns and pd.api.types.is_datetime64_any_dtype(df['Latest_Launch']):
    # For EDA purposes, we might not treat datetime as categorical for countplots
    # but you could extract year/month if needed:
    # df['Launch_Year'] = df['Latest_Launch'].dt.year
    # categorical_cols.append('Launch_Year')
    pass # Keep it as datetime for now

for col in categorical_cols:
    if col == 'Model': # Model names are too unique for a meaningful count plot
        print(f"\nValue counts for {col} (Top 10):")
        print(df[col].value_counts().head(10))
        print(f"... and {df[col].nunique() - 10} more unique models.")
        continue

    print(f"\nValue counts for {col}:")
    print(df[col].value_counts())

    # Plot if number of unique values is manageable
    if df[col].nunique() < 30 and df[col].nunique() > 1: # Ensure there's something to plot
        plt.figure(figsize=(12, max(6, df[col].nunique() * 0.3))) # Adjust height based on num categories
        sns.countplot(y=df[col], order=df[col].value_counts().index, palette="viridis")
        plt.title(f"Frequency of {col}", fontsize=14)
        plt.xlabel("Count", fontsize=12)
        plt.ylabel(col, fontsize=12)
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=10)
        plt.tight_layout()
        plt.show()
    elif df[col].nunique() == 1:
        print(f"Skipping plot for {col} as it has only 1 unique value.")
    else:
        print(f"Skipping plot for {col} due to too many unique values ({df[col].nunique()}). Consider top N if needed.")

# Correlation heatmap for numerical features
print("\nPlotting correlation heatmap...")
plt.figure(figsize=(12, 10))
# Ensure only numeric columns are used for correlation
numeric_df_for_corr = df[numeric_cols]
correlation_matrix = numeric_df_for_corr.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix of Numerical Features', fontsize=15)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# Scatter plots for interesting relationships
print("\nPlotting scatter plots for key relationships...")
scatter_pairs = [
    ('Horsepower', 'Price_in_thousands'),
    ('Engine_size', 'Horsepower'),
    ('Curb_weight', 'Fuel_efficiency'),
    ('Price_in_thousands', 'Year_resale_value'),
    ('Sales_in_thousands', 'Price_in_thousands')
]

plt.figure(figsize=(18, 10))
for i, (x_col, y_col) in enumerate(scatter_pairs):
    if x_col in df.columns and y_col in df.columns:
        plt.subplot(2, 3, i + 1) # Adjust grid as needed
        sns.scatterplot(x=df[x_col], y=df[y_col], hue=df.get('Vehicle_type', None), alpha=0.7)
        plt.title(f'{y_col} vs. {x_col}', fontsize=12)
        plt.xlabel(x_col, fontsize=10)
        plt.ylabel(y_col, fontsize=10)
        plt.xticks(fontsize=8)
        plt.yticks(fontsize=8)
        if df.get('Vehicle_type', None) is not None:
            plt.legend(title='Vehicle Type', fontsize=8, title_fontsize=10)
    else:
        print(f"Warning: Columns {x_col} or {y_col} not found for scatter plot.")
plt.suptitle('Scatter Plots of Key Numerical Relationships', y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

# Grouped analysis: Average Price by Manufacturer (Top N)
if 'Manufacturer' in df.columns and 'Price_in_thousands' in df.columns:
    print("\nAnalyzing average price by Manufacturer (Top 15 by count)...")
    top_manufacturers = df['Manufacturer'].value_counts().nlargest(15).index
    avg_price_by_manufacturer = df[df['Manufacturer'].isin(top_manufacturers)].groupby('Manufacturer')['Price_in_thousands'].mean().sort_values(ascending=False)

    plt.figure(figsize=(12, 8))
    sns.barplot(x=avg_price_by_manufacturer.values, y=avg_price_by_manufacturer.index, palette="mako")
    plt.title('Average Price (in thousands) by Manufacturer (Top 15)', fontsize=15)
    plt.xlabel('Average Price (in thousands)', fontsize=12)
    plt.ylabel('Manufacturer', fontsize=12)
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)
    plt.tight_layout()
    plt.show()

# Grouped analysis: Average Horsepower by Vehicle Type
if 'Vehicle_type' in df.columns and 'Horsepower' in df.columns:
    print("\nAnalyzing average horsepower by Vehicle Type...")
    avg_hp_by_vehicle_type = df.groupby('Vehicle_type')['Horsepower'].mean().sort_values(ascending=False)

    plt.figure(figsize=(8, 5))
    sns.barplot(x=avg_hp_by_vehicle_type.index, y=avg_hp_by_vehicle_type.values, palette="crest")
    plt.title('Average Horsepower by Vehicle Type', fontsize=15)
    plt.xlabel('Vehicle Type', fontsize=12)
    plt.ylabel('Average Horsepower', fontsize=12)
    plt.xticks(fontsize=10, rotation=45, ha='right')
    plt.yticks(fontsize=10)
    plt.tight_layout()
    plt.show()

print("\n--- EDA Complete ---")

"""This exploratory data analysis of the Car Sales dataset involved substantial cleaning (addressing missing values, type inconsistencies, and duplicates), yielding a refined dataset of 155 unique vehicle models.
1. Key findings reveal:
Price Drivers: Vehicle price strongly correlates positively with horsepower, engine size, and curb weight.
Fuel Efficiency Trade-off: Fuel efficiency is inversely related to these same factors (heavier, more powerful cars are less efficient).
2. Market Distribution: Sales, price, and horsepower distributions are right-skewed, indicating most cars fall into lower/moderate ranges, with distinct high-performance or luxury outliers.
3. Manufacturer Presence: Ford, Dodge, and Chevrolet are the most represented manufacturers.
Vehicle Type Differences: "Passenger" vehicles (sedans/coupes) are more common. "Car" types (likely SUVs/trucks) generally exhibit higher power and lower fuel efficiency.
4. Luxury brands (e.g., Mercedes-Benz, Porsche) command significantly higher average prices.

Limitations: The dataset primarily reflects models launched around 2011-2012. Significant imputation was required for Year_resale_value, which should be considered in its interpretation.
"""

